//! This module contains utility functions for the LLM module

// Use the tokenizer data generated in build.rs and included through tokenizer_data.rs
pub use crate::tokenizer_data::{TOKENIZER_JSON as EMBEDDED_TOKENIZER_JSON, CONFIG_JSON as EMBEDDED_CONFIG_JSON};

// Note: We no longer use EMBEDDED_MODEL_SAFETENSORS as we directly
// embed the GGUF model with include_bytes! in embedded_model.rs
// This is kept as an empty array for backward compatibility with existing code
/// Legacy placeholder for embedded model data - now unused as the model is included directly
pub static EMBEDDED_MODEL_SAFETENSORS: &[u8] = &[];

/// Extracts the assistant's response from the generated text
pub fn extract_response(generated_text: &str) -> String {
    println!("Extracting response from output text ({} chars)...", generated_text.len());
    
    // For newly generated text (just the assistant's response)
    // Simply clean the message of special tokens and return it
    let cleaned = clean_message(generated_text);
    dedup_repeating_words(&cleaned).trim().to_string()
}

/// Remove repetitive words that might be generated by the model
fn dedup_repeating_words(text: &str) -> String {
    let mut deduped = String::new();
    let mut last_word = "";
    let mut repeat_count = 0;
    
    // Split by whitespace and process each word
    for word in text.split_whitespace() {
        if word == last_word {
            repeat_count += 1;
            // Only allow up to 2 repetitions of the same word
            if repeat_count > 2 {
                continue;
            }
        } else {
            repeat_count = 0;
        }
        
        if !deduped.is_empty() {
            deduped.push(' ');
        }
        deduped.push_str(word);
        last_word = word;
    }
    
    deduped
}


/// Cleans a message by removing protocol markers and special tokens
fn clean_message(text: &str) -> String {
    // List of all markers to remove for clean output
    let markers = [
        // ChatML markers for Qwen3
        "<|im_start|>", "<|im_end|>", 
        "<|endoftext|>",
        
        // Role markers
        "<|system|>", "<|user|>", "<|assistant|>",
        "system:", "user:", "assistant:",
        
        // Thinking markers
        "<think>", "</think>",
        
        // Encoded markers that might appear in tokenizer output
        "ccimcstartcc", "ccimcendcc", 
        "ccsystemc", "ccuserc", "ccassistantc",
        
        // Misc artifacts
        "Pb", "systemYouare",
        
        // Tool markers
        "<tool_call>", "</tool_call>",
        "<tool_response>", "</tool_response>",
        
        // Remnants of special tokens being decoded badly by previous tokenizer
        "Ĵ", "ĵ", "Ĝ", "ĝ", "Ħ", "ħ",
        
        // Spacing inconsistencies from improper merges
        "Ġ", "▁", "##"
    ];
    
    // Remove all markers
    let mut clean = text.to_string();
    for marker in markers {
        clean = clean.replace(marker, "");
    }
    
    // Normalize multiple spaces/newlines that can happen with BPE decoding
    let re_whitespace = regex::Regex::new(r"\s+").unwrap();
    clean = re_whitespace.replace_all(&clean, " ").to_string();
    
    // Normalize newlines
    let re_newlines = regex::Regex::new(r"\n\s*\n+").unwrap();
    clean = re_newlines.replace_all(&clean, "\n\n").to_string();
    
    // Remove any non-content lines and empty lines
    clean.lines()
        .filter(|line| !line.trim().is_empty())
        .collect::<Vec<_>>()
        .join("\n")
        .trim()
        .to_string()
}

/// Debugging helper to parse a formatted prompt string to see how it's structured
pub fn debug_parse_prompt(prompt: &str) -> String {
    let mut result = String::new();
    
    // Look for all ChatML markers and print their positions
    let markers = [
        "<|im_start|>system", 
        "<|im_end|>", 
        "<|im_start|>user", 
        "<|im_start|>assistant",
        "<|endoftext|>"
    ];
    
    for marker in &markers {
        if let Some(pos) = prompt.find(marker) {
            result.push_str(&format!("Found '{}' at position {}\n", marker, pos));
        } else {
            result.push_str(&format!("Marker '{}' not found\n", marker));
        }
    }
    
    // Also try to extract parts by role and print their content
    if let Some(start) = prompt.find("<|im_start|>system") {
        if let Some(end_offset) = prompt[start..].find("<|im_end|>") {
            let content_start = start + "<|im_start|>system".len();
            let system_msg = &prompt[content_start..start+end_offset];
            result.push_str(&format!("\nSystem message:\n{}\n", system_msg.trim()));
        }
    }
    
    if let Some(start) = prompt.find("<|im_start|>user") {
        if let Some(end_offset) = prompt[start..].find("<|im_end|>") {
            let content_start = start + "<|im_start|>user".len();
            let user_msg = &prompt[content_start..start+end_offset];
            result.push_str(&format!("\nUser message:\n{}\n", user_msg.trim()));
        }
    }
    
    result
}

/// Simple helper for loading an embedded model from bytes to temporary file
/// This is now using the direct GGUF model bytes from embedded_model.rs
pub fn get_embedded_model_path() -> std::io::Result<String> {
    use std::env::temp_dir;
    use std::fs::File;
    use std::io::Write;
    use std::path::Path;
    
    // Create a temporary file with a consistent name to store the model
    // We'll include a suffix indicating this is the F16 model to differentiate from other formats
    let mut temp_path = temp_dir();
    temp_path.push("arkavo_embedded_model_f16.gguf");
    
    // Check if file exists with the correct size
    let should_write = if temp_path.exists() {
        match std::fs::metadata(&temp_path) {
            Ok(metadata) => {
                // Verify file size matches embedded model size
                if metadata.len() as usize != crate::EMBEDDED_MODEL.len() {
                    println!("Temporary file exists but has wrong size, recreating");
                    true
                } else {
                    println!("Using existing temporary file for GGUF model: {:?}", temp_path);
                    false
                }
            },
            Err(_) => true
        }
    } else {
        true
    };
    
    // Write the model to the temporary file if needed
    if should_write {
        println!("Writing embedded GGUF model to temporary file: {:?}", temp_path);
        
        // Ensure parent directory exists
        if let Some(parent) = Path::new(&temp_path).parent() {
            if !parent.exists() {
                std::fs::create_dir_all(parent)?;
            }
        }
        
        let mut file = File::create(&temp_path)?;
        file.write_all(crate::EMBEDDED_MODEL)?;
        println!("Successfully wrote {} bytes to temporary file", crate::EMBEDDED_MODEL.len());
    }
    
    Ok(temp_path.to_string_lossy().to_string())
}