//! This module contains utility functions for the LLM module

// Use the tokenizer data generated in build.rs and included through tokenizer_data.rs
pub use crate::tokenizer_data::{TOKENIZER_JSON as EMBEDDED_TOKENIZER_JSON, CONFIG_JSON as EMBEDDED_CONFIG_JSON};

// Note: We no longer use EMBEDDED_MODEL_SAFETENSORS as we directly
// embed the GGUF model with include_bytes! in embedded_model.rs
// This is kept as an empty array for backward compatibility with existing code
/// Legacy placeholder for embedded model data - now unused as the model is included directly
pub static EMBEDDED_MODEL_SAFETENSORS: &[u8] = &[];

/// Extracts the assistant's response from the generated text
pub fn extract_response(generated_text: &str) -> String {
    // For Qwen3 response extraction, following Hugging Face's implementation
    
    // These checks are only used for context and don't affect control flow
    // so we prefix them with _ to avoid clippy warnings
    
    // First, check for thinking mode (not currently used but prepared for future)
    if generated_text.contains("<think>") && generated_text.contains("</think>") {
        if let Some(think_end) = generated_text.rfind("</think>") {
            // Extract post-thinking content (after the last </think> tag)
            let remaining = &generated_text[think_end + "</think>".len()..];
            if !remaining.trim().is_empty() {
                return remaining.trim().to_string();
            }
        }
    }

    // Prefer extracting from last assistant block
    if let Some(start_idx) = generated_text.rfind("<|im_start|>assistant") {
        let content_start = start_idx + "<|im_start|>assistant".len();
        let content_start = if content_start < generated_text.len() && 
                            generated_text[content_start..].starts_with('\n') {
            content_start + 1 // Skip the newline
        } else {
            content_start
        };
        if let Some(end_idx) = generated_text[content_start..].find("<|im_end|>") {
            let assistant_text = &generated_text[content_start..content_start+end_idx];
            let clean_text = clean_message(assistant_text);
            
            // Remove repeating words that might happen during generation
            let clean_text = dedup_repeating_words(&clean_text);
            
            return clean_text.trim().to_string();
        } else {
            // No end tag found, just take the rest after assistant tag
            let assistant_text = &generated_text[content_start..];
            let clean_text = clean_message(assistant_text);
            let clean_text = dedup_repeating_words(&clean_text);
            
            return clean_text.trim().to_string();
        }
    }
    
    // Fallback: get text after last <|im_end|> (the user's block)
    if let Some(pos) = generated_text.rfind("<|im_end|>") {
        let after = &generated_text[pos + "<|im_end|>".len()..];
        if let Some(asst_pos) = after.find("<|im_start|>assistant") {
            let asst_content = &after[asst_pos + "<|im_start|>assistant".len()..];
            let clean_text = clean_message(asst_content);
            return dedup_repeating_words(&clean_text).trim().to_string();
        }
        
        // If there's no assistant marker after the last im_end, just take everything
        if !after.trim().is_empty() {
            let clean_text = clean_message(after);
            return dedup_repeating_words(&clean_text).trim().to_string();
        }
    }
    
    // Check for simple colon-based format
    if let Some(asst_pos) = generated_text.find("Assistant:") {
        let content_start = asst_pos + "Assistant:".len();
        let clean_text = clean_message(&generated_text[content_start..]);
        return dedup_repeating_words(&clean_text).trim().to_string();
    }
    
    if let Some(asst_pos) = generated_text.find("assistant:") {
        let content_start = asst_pos + "assistant:".len();
        let clean_text = clean_message(&generated_text[content_start..]);
        return dedup_repeating_words(&clean_text).trim().to_string();
    }
    
    // Last resort: use our structured extraction
    let parts = extract_conversation_parts(generated_text);
    if let Some(assistant_response) = parts.assistant_response {
        return dedup_repeating_words(&assistant_response).trim().to_string();
    }
    
    // Final fallback: clean up all protocol tokens
    let cleaned = clean_message(generated_text);
    let result = dedup_repeating_words(&cleaned).trim().to_string();
    result
}

/// Remove repetitive words that might be generated by the model
fn dedup_repeating_words(text: &str) -> String {
    let mut deduped = String::new();
    let mut last_word = "";
    let mut repeat_count = 0;
    
    // Split by whitespace and process each word
    for word in text.split_whitespace() {
        if word == last_word {
            repeat_count += 1;
            // Only allow up to 2 repetitions of the same word
            if repeat_count > 2 {
                continue;
            }
        } else {
            repeat_count = 0;
        }
        
        if !deduped.is_empty() {
            deduped.push(' ');
        }
        deduped.push_str(word);
        last_word = word;
    }
    
    deduped
}

/// Breaks down a conversation into system, user, and assistant parts
struct ConversationParts {
    /// System message content
    pub system_message: Option<String>,
    /// User message content 
    pub user_message: Option<String>,
    /// Assistant response content
    pub assistant_response: Option<String>,
}

/// Extracts conversation parts (system, user, assistant) from raw text
fn extract_conversation_parts(text: &str) -> ConversationParts {
    let mut parts = ConversationParts {
        system_message: None,
        user_message: None,
        assistant_response: None,
    };
    
    // Look for patterns that indicate message boundaries
    
    // Pattern: <|im_start|>system ... <|im_end|>
    if let Some(start) = text.find("<|im_start|>system") {
        if let Some(end) = text[start..].find("<|im_end|>") {
            let content_start = start + "<|im_start|>system".len();
            // Skip newline if present
            let content_start = if content_start < text.len() && text[content_start..].starts_with('\n') {
                content_start + 1
            } else {
                content_start
            };
            let content = text[content_start..start+end].trim();
            parts.system_message = Some(content.to_string());
        }
    }
    
    // Pattern: <|im_start|>user ... <|im_end|>
    if let Some(start) = text.find("<|im_start|>user") {
        if let Some(end) = text[start..].find("<|im_end|>") {
            let content_start = start + "<|im_start|>user".len();
            // Skip newline if present
            let content_start = if content_start < text.len() && text[content_start..].starts_with('\n') {
                content_start + 1
            } else {
                content_start
            };
            let content = text[content_start..start+end].trim();
            parts.user_message = Some(content.to_string());
        }
    }
    
    // Pattern: <|im_start|>assistant ... <|im_end|>
    if let Some(start) = text.find("<|im_start|>assistant") {
        if let Some(end) = text[start..].find("<|im_end|>") {
            let content_start = start + "<|im_start|>assistant".len();
            // Skip newline if present
            let content_start = if content_start < text.len() && text[content_start..].starts_with('\n') {
                content_start + 1
            } else {
                content_start
            };
            let content = text[content_start..start+end].trim();
            parts.assistant_response = Some(content.to_string());
        }
    }
    
    // Also check for alternative formats
    
    // Check for "system:" format
    if parts.system_message.is_none() {
        if let Some(start) = text.find("system:") {
            let content_start = start + "system:".len();
            // Find the next role marker
            let end = text[content_start..].find("user:").or_else(|| text[content_start..].find("assistant:"))
                .map(|pos| content_start + pos)
                .unwrap_or(text.len());
            
            let content = text[content_start..end].trim();
            if !content.is_empty() {
                parts.system_message = Some(content.to_string());
            }
        }
    }
    
    // Check for "user:" format
    if parts.user_message.is_none() {
        if let Some(start) = text.find("user:") {
            let content_start = start + "user:".len();
            // Find the next role marker
            let end = text[content_start..].find("assistant:").or_else(|| text[content_start..].find("system:"))
                .map(|pos| content_start + pos)
                .unwrap_or(text.len());
            
            let content = text[content_start..end].trim();
            if !content.is_empty() {
                parts.user_message = Some(content.to_string());
            }
        }
    }
    
    // Check for "assistant:" format
    if parts.assistant_response.is_none() {
        if let Some(start) = text.find("assistant:") {
            let content_start = start + "assistant:".len();
            // Find the next role marker (or end of text)
            let end = text[content_start..].find("user:").or_else(|| text[content_start..].find("system:"))
                .map(|pos| content_start + pos)
                .unwrap_or(text.len());
            
            let content = text[content_start..end].trim();
            if !content.is_empty() {
                parts.assistant_response = Some(content.to_string());
            }
        }
    }
    
    // If all else fails, check for user query and what follows
    if parts.assistant_response.is_none() && parts.user_message.is_some() {
        let user_message = parts.user_message.as_ref().unwrap();
        if let Some(pos) = text.find(user_message) {
            let after_user = text[pos + user_message.len()..].trim();
            if !after_user.is_empty() {
                parts.assistant_response = Some(after_user.to_string());
            }
        }
    }
    
    // Cleanup all parts
    if let Some(ref mut system) = parts.system_message {
        *system = clean_message(system);
    }
    
    if let Some(ref mut user) = parts.user_message {
        *user = clean_message(user);
    }
    
    if let Some(ref mut assistant) = parts.assistant_response {
        *assistant = clean_message(assistant);
    }
    
    parts
}

/// Cleans a message by removing protocol markers and special tokens
fn clean_message(text: &str) -> String {
    // List of all markers to remove for clean output
    let markers = [
        // ChatML markers for Qwen3
        "<|im_start|>", "<|im_end|>", 
        "<|endoftext|>",
        
        // Role markers
        "<|system|>", "<|user|>", "<|assistant|>",
        "system:", "user:", "assistant:",
        
        // Thinking markers
        "<think>", "</think>",
        
        // Encoded markers that might appear in tokenizer output
        "ccimcstartcc", "ccimcendcc", 
        "ccsystemc", "ccuserc", "ccassistantc",
        
        // Misc artifacts
        "Pb", "systemYouare",
        
        // Tool markers
        "<tool_call>", "</tool_call>",
        "<tool_response>", "</tool_response>",
        
        // Remnants of special tokens being decoded badly by previous tokenizer
        "Ĵ", "ĵ", "Ĝ", "ĝ", "Ħ", "ħ",
        
        // Spacing inconsistencies from improper merges
        "Ġ", "▁", "##"
    ];
    
    // Remove all markers
    let mut clean = text.to_string();
    for marker in markers {
        clean = clean.replace(marker, "");
    }
    
    // Normalize multiple spaces/newlines that can happen with BPE decoding
    let re_whitespace = regex::Regex::new(r"\s+").unwrap();
    clean = re_whitespace.replace_all(&clean, " ").to_string();
    
    // Normalize newlines
    let re_newlines = regex::Regex::new(r"\n\s*\n+").unwrap();
    clean = re_newlines.replace_all(&clean, "\n\n").to_string();
    
    // Remove any non-content lines and empty lines
    clean.lines()
        .filter(|line| !line.trim().is_empty())
        .collect::<Vec<_>>()
        .join("\n")
        .trim()
        .to_string()
}

/// Debugging helper to parse a formatted prompt string to see how it's structured
pub fn debug_parse_prompt(prompt: &str) -> String {
    let mut result = String::new();
    
    // Look for all ChatML markers and print their positions
    let markers = [
        "<|im_start|>system", 
        "<|im_end|>", 
        "<|im_start|>user", 
        "<|im_start|>assistant",
        "<|endoftext|>"
    ];
    
    for marker in &markers {
        if let Some(pos) = prompt.find(marker) {
            result.push_str(&format!("Found '{}' at position {}\n", marker, pos));
        } else {
            result.push_str(&format!("Marker '{}' not found\n", marker));
        }
    }
    
    // Also try to extract parts by role and print their content
    if let Some(start) = prompt.find("<|im_start|>system") {
        if let Some(end_offset) = prompt[start..].find("<|im_end|>") {
            let content_start = start + "<|im_start|>system".len();
            let system_msg = &prompt[content_start..start+end_offset];
            result.push_str(&format!("\nSystem message:\n{}\n", system_msg.trim()));
        }
    }
    
    if let Some(start) = prompt.find("<|im_start|>user") {
        if let Some(end_offset) = prompt[start..].find("<|im_end|>") {
            let content_start = start + "<|im_start|>user".len();
            let user_msg = &prompt[content_start..start+end_offset];
            result.push_str(&format!("\nUser message:\n{}\n", user_msg.trim()));
        }
    }
    
    result
}

/// Simple helper for loading an embedded model from bytes to temporary file
/// This is now using the direct GGUF model bytes from embedded_model.rs
pub fn get_embedded_model_path() -> std::io::Result<String> {
    use std::env::temp_dir;
    use std::fs::File;
    use std::io::Write;
    use std::path::Path;
    
    // Create a temporary file with a consistent name to store the model
    // We'll include a suffix indicating this is the F16 model to differentiate from other formats
    let mut temp_path = temp_dir();
    temp_path.push("arkavo_embedded_model_f16.gguf");
    
    // Check if file exists with the correct size
    let should_write = if temp_path.exists() {
        match std::fs::metadata(&temp_path) {
            Ok(metadata) => {
                // Verify file size matches embedded model size
                if metadata.len() as usize != crate::EMBEDDED_MODEL.len() {
                    println!("Temporary file exists but has wrong size, recreating");
                    true
                } else {
                    println!("Using existing temporary file for GGUF model: {:?}", temp_path);
                    false
                }
            },
            Err(_) => true
        }
    } else {
        true
    };
    
    // Write the model to the temporary file if needed
    if should_write {
        println!("Writing embedded GGUF model to temporary file: {:?}", temp_path);
        
        // Ensure parent directory exists
        if let Some(parent) = Path::new(&temp_path).parent() {
            if !parent.exists() {
                std::fs::create_dir_all(parent)?;
            }
        }
        
        let mut file = File::create(&temp_path)?;
        file.write_all(crate::EMBEDDED_MODEL)?;
        println!("Successfully wrote {} bytes to temporary file", crate::EMBEDDED_MODEL.len());
    }
    
    Ok(temp_path.to_string_lossy().to_string())
}